name: CI - Quality Gate

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  quality-gate:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ['3.11', '3.12']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install flake8 black pip-audit

    - name: Linting (flake8) - syntax errors and undefined names
      run: |
        flake8 execution/ --count --select=E9,F63,F7,F82 --show-source --statistics

    - name: Formatting check (black)
      run: |
        black --check execution/ tests/

    - name: Security - secrets scan
      run: |
        echo "Scanning for exposed secrets..."
        FOUND=0
        # OpenAI API keys
        if grep -rn "sk-[a-zA-Z0-9]\{20,\}" execution/ tests/ --include="*.py" 2>/dev/null; then
          echo "CRITICAL: Potential API key found in source code"
          FOUND=1
        fi
        # Generic secrets patterns
        if grep -rn "password\s*=\s*['\"][^'\"]\+" execution/ tests/ --include="*.py" 2>/dev/null | grep -v "getenv\|environ\|os\.get\|\.env\|example\|test\|mock\|fake\|dummy"; then
          echo "WARNING: Potential hardcoded password found"
          FOUND=1
        fi
        if [ "$FOUND" -eq 1 ]; then
          exit 1
        fi
        echo "No secrets detected"

    - name: Security - dependency audit
      run: |
        pip-audit --strict

    - name: Unit tests
      run: |
        if find tests/unit -name "*.py" -not -name "__init__.py" 2>/dev/null | grep -q .; then
          pytest tests/unit/ -v --cov=execution --cov-report=xml --cov-report=term --cov-fail-under=80
        else
          echo "No unit tests found yet. Skipping."
        fi

    - name: Integration tests
      run: |
        if find tests/integration -name "*.py" -not -name "__init__.py" 2>/dev/null | grep -q .; then
          pytest tests/integration/ -v
        else
          echo "No integration tests found yet. Skipping."
        fi
      env:
        ENVIRONMENT: test

    - name: LLM evals (Golden Dataset)
      run: |
        if [ -f "execution/run_evals.py" ] && [ -f "tests/evals/golden_dataset.json" ]; then
          python execution/run_evals.py
        else
          echo "Evals not configured yet. Skipping."
        fi

    - name: Quality Gate summary
      if: success()
      run: |
        echo "========================================="
        echo "ALL CHECKS PASSED - Ready to merge"
        echo "========================================="

    - name: Quality Gate failed
      if: failure()
      run: |
        echo "========================================="
        echo "QUALITY GATE FAILED"
        echo "Fix issues locally before pushing again."
        echo "========================================="
        exit 1
