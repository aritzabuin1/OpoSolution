# ADR: Modelo de Embeddings ‚Äî OpenAI text-embedding-3-small

**ID**: ADR-0010
**Fecha**: 2026-02-15
**Estado**: üü¢ Aceptado
**Autor**: Claude / Aritz

## 1. Contexto y Problema

El RAG pipeline de OPTEK necesita convertir chunks de legislaci√≥n y queries de usuarios en vectores num√©ricos para b√∫squeda sem√°ntica. Necesitamos un modelo de embeddings que ofrezca buena calidad sem√°ntica para texto jur√≠dico espa√±ol, dimensionalidad razonable para pgvector, y coste bajo para ingesta masiva (~5.000-15.000 chunks iniciales + queries de b√∫squeda en cada request).

## 2. Opciones Consideradas

- **Opci√≥n A**: OpenAI text-embedding-3-small ‚Äî 1536 dimensiones, multiling√ºe (buen rendimiento en espa√±ol), ~$0.02/M tokens. Modelo maduro y bien benchmarkeado.
- **Opci√≥n B**: OpenAI text-embedding-3-large ‚Äî 3072 dimensiones, mayor precisi√≥n pero 6.5x m√°s caro (~$0.13/M tokens) y vectores m√°s grandes en BD.
- **Opci√≥n C**: Voyage AI (voyage-3) ‚Äî Competitivo en benchmarks, 1024 dimensiones, buen soporte multiling√ºe. Menos maduro que OpenAI.
- **Opci√≥n D**: Embeddings open-source autoalojado (e5-multilingual, BGE-M3) ‚Äî Sin coste de API pero requiere infraestructura GPU. Mayor complejidad operativa.
- **Opci√≥n E**: Anthropic embeddings ‚Äî A fecha de escritura, Anthropic no ofrece un modelo de embeddings dedicado. No es opci√≥n viable.

## 3. Decisi√≥n Elegida: OpenAI text-embedding-3-small

**Justificaci√≥n T√©cnica**:

1. **Coste m√≠nimo**: ~$0.02/M tokens. La ingesta completa de legislaci√≥n (~15.000 chunks √ó ~500 tokens = 7.5M tokens) cuesta ~$0.15. Las queries de b√∫squeda son negligibles (~100 tokens √ó queries/d√≠a).
2. **Calidad suficiente**: text-embedding-3-small rinde bien en benchmarks MTEB para espa√±ol. Para nuestro caso (buscar art√≠culos relevantes dado un tema), no necesitamos la m√°xima precisi√≥n ‚Äî el contexto RAG se complementa con filtros SQL (oposicion_id, tema_id).
3. **Dimensionalidad equilibrada**: 1536 dimensiones ofrece buen rendimiento sem√°ntico sin sobrecargar pgvector. El √≠ndice HNSW es eficiente a esta dimensi√≥n.
4. **API estable**: OpenAI embeddings API es el est√°ndar de facto, con SDKs maduros y uptime >99.9%.
5. **Multiling√ºe nativo**: Buen rendimiento en espa√±ol sin fine-tuning. Importante dado que toda la legislaci√≥n est√° en castellano.

## 4. Consecuencias (Trade-offs)

- **Positivas**: Coste casi negligible (<$1/mes incluso con 1000 usuarios), calidad probada en espa√±ol, API estable, f√°cil de implementar (1 endpoint).
- **Negativas/Riesgos**: Introduce dependencia de OpenAI adem√°s de Anthropic (2 proveedores de IA). Mitigaci√≥n: los embeddings son un commodity ‚Äî se pueden regenerar con cualquier modelo cambiando una funci√≥n. Si OpenAI cambia pricing o depreca el modelo, la migraci√≥n es un script de reingesta (ver ADR-0002, los embeddings est√°n en pgvector). Latencia adicional para query embedding (~50-100ms por b√∫squeda).

## 5. Notas de Implementaci√≥n

- SDK: `openai` package, solo para `client.embeddings.create()`.
- Funci√≥n wrapper: `/lib/ai/embeddings.ts` ‚Üí `generateEmbedding(text: string): Promise<number[]>`.
- Ingesta: `execution/ingest-legislacion.ts` genera embeddings para cada chunk y los almacena en columna `embedding vector(1536)`.
- Query-time: En cada generaci√≥n de test, se embedea la query del usuario y se busca con `match_legislacion()` RPC.
- Secret: `OPENAI_API_KEY` en `.env.local` (solo para embeddings, no para generaci√≥n).
- Monitorizaci√≥n: loguear tokens consumidos en `monitoring/COSTS.md`.
- Migraci√≥n futura: si Anthropic lanza embeddings o surge alternativa mejor, cambiar solo `generateEmbedding()` y reejecutar `ingest-legislacion.ts`.
